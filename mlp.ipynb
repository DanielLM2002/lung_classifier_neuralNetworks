{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, ops\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'COVID-19_Radiography_Dataset'\n",
    "train_dir = os.path.join(root_dir, 'lbp', 'train')\n",
    "val_dir = os.path.join(root_dir, 'lbp', 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdlizano\u001b[0m (\u001b[33mci-0148-g3\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\ISRAEL\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\ISRAEL\\Documents\\Estudios\\UCR\\Computacion\\MachineLearning\\proyecto2\\lung_classifier_neuralNetworks\\wandb\\run-20240527_014639-hkgtf0in</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ci-0148-g3/mlp_local_binary_patterns/runs/hkgtf0in' target=\"_blank\">LBP default</a></strong> to <a href='https://wandb.ai/ci-0148-g3/mlp_local_binary_patterns' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ci-0148-g3/mlp_local_binary_patterns' target=\"_blank\">https://wandb.ai/ci-0148-g3/mlp_local_binary_patterns</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ci-0148-g3/mlp_local_binary_patterns/runs/hkgtf0in' target=\"_blank\">https://wandb.ai/ci-0148-g3/mlp_local_binary_patterns/runs/hkgtf0in</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ci-0148-g3/mlp_local_binary_patterns/runs/hkgtf0in?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x278670e0e50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=os.getenv('WANDB_KEY'))\n",
    "wandb.init(project='mlp_local_binary_patterns', name='LBP default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script shown below calculates the mean and standard deviation for the training set. These values are then used to apply the standarization to the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(train_dir, transform=Grayscale())\n",
    "loader = DataLoader(dataset, 64, shuffle=True)\n",
    "\n",
    "# Initialize variables for mean and std calculation\n",
    "mean = 0.0\n",
    "std = 0.0\n",
    "nb_samples = 0\n",
    "\n",
    "for data, _ in loader:\n",
    "    data = data.to(device)\n",
    "    batch_samples = data.size(0)  # number of images in the batch\n",
    "    data = data.view(batch_samples, -1)  # flatten the channel and spatial dimensions\n",
    "    mean += data.mean(1).sum(0)\n",
    "    std += data.std(1).sum(0)\n",
    "    nb_samples += batch_samples\n",
    "\n",
    "mean /= nb_samples\n",
    "std /= nb_samples\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These transforms are applied for standarization and data augmentation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "        transforms.RandomResizedCrop(size=299, scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=0.0197, std=0.0083)  # Normalize the image (uniform LBP)\n",
    "        transforms.Normalize(mean=0.6133, std=0.3372)  # Normalize the image (default LBP)\n",
    "        # transforms.Normalize(mean=0.2283, std=0.3103)  # Normalize the image (ror LBP)\n",
    "        # transforms.Normalize(mean=0.0772, std=0.1784)  # Normalize the image (var LBP)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Grayscale(),\n",
    "        # transforms.Normalize(mean=0.0197, std=0.0083)  # Normalize the (uniform LBP)\n",
    "        transforms.Normalize(mean=0.6133, std=0.3372)  # Normalize the image (default LBP)\n",
    "        # transforms.Normalize(mean=0.2283, std=0.3103)  # Normalize the image (ror LBP)\n",
    "        # transforms.Normalize(mean=0.0772, std=0.1784)  # Normalize the image (var LBP)\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP PyTorch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim, hidden_layers=[128], activation='relu', dropout=0):\n",
    "    super(MLP, self).__init__()\n",
    "\n",
    "    if activation == \"relu\":\n",
    "      activation_func = lambda: nn.ReLU()\n",
    "    elif activation == \"prelu\":\n",
    "      activation_func = lambda: nn.PReLU()\n",
    "    # You can add other activation functions here\n",
    "    else:\n",
    "      raise NotImplementedError(\"Activation function not supported\")\n",
    "\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.input_layer = nn.Linear(input_dim, hidden_layers[0])\n",
    "    self.input_activation = activation_func()\n",
    "\n",
    "    self.hidden_layers = nn.ModuleList([])\n",
    "    for index in range(len(hidden_layers)):\n",
    "        next = index + 1\n",
    "        if next < len(hidden_layers):\n",
    "          self.hidden_layers.append(nn.Linear(hidden_layers[index], hidden_layers[next]))\n",
    "          self.hidden_layers.append(nn.BatchNorm1d(hidden_layers[next]))\n",
    "        else:\n",
    "          self.hidden_layers.append(nn.Linear(hidden_layers[index], hidden_layers[index]))\n",
    "          self.hidden_layers.append(nn.BatchNorm1d(hidden_layers[index]))\n",
    "        self.hidden_layers.append(activation_func())\n",
    "        if dropout > 0:\n",
    "          self.hidden_layers.append(nn.Dropout(dropout))\n",
    "\n",
    "    self.output_layer = nn.Linear(hidden_layers[-1], output_dim)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.flatten(x)\n",
    "    x = self.input_layer(x)\n",
    "    x = self.input_activation(x)\n",
    "\n",
    "    for layer in self.hidden_layers:\n",
    "      x = layer(x)\n",
    "\n",
    "    x = self.output_layer(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions and initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "input_features = 299 * 299\n",
    "hidden_layers = [256, 256]\n",
    "num_classes = 4\n",
    "dropout = 0.5\n",
    "activation = 'prelu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=data_transforms['train'])\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=data_transforms['val'])\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "model = MLP(input_dim=input_features,\n",
    "            hidden_layers=hidden_layers,\n",
    "            output_dim=num_classes,\n",
    "            dropout=dropout,\n",
    "            activation=activation).to(device)\n",
    "\n",
    "# Define loss function and optimizer (replace with your choices)\n",
    "criterion = nn.CrossEntropyLoss()  # Assuming classification task\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Training: 100%|██████████| 265/265 [02:18<00:00,  1.92batch/s, Loss=1.03, Acc=0.551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train Loss: 1.0295 Acc: 0.5514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Validation: 100%|██████████| 67/67 [00:16<00:00,  3.97batch/s, Loss=0.853, Acc=0.648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.8532 Acc: 0.6482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Training: 100%|██████████| 265/265 [02:10<00:00,  2.03batch/s, Loss=0.896, Acc=0.626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "Train Loss: 0.8961 Acc: 0.6259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Validation: 100%|██████████| 67/67 [00:17<00:00,  3.78batch/s, Loss=0.81, Acc=0.667] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.8100 Acc: 0.6673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Training: 100%|██████████| 265/265 [02:11<00:00,  2.02batch/s, Loss=0.825, Acc=0.667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n",
      "Train Loss: 0.8250 Acc: 0.6668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Validation: 100%|██████████| 67/67 [00:16<00:00,  4.01batch/s, Loss=0.693, Acc=0.726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6933 Acc: 0.7263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Training: 100%|██████████| 265/265 [02:27<00:00,  1.80batch/s, Loss=0.782, Acc=0.684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n",
      "Train Loss: 0.7822 Acc: 0.6842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Validation: 100%|██████████| 67/67 [00:21<00:00,  3.16batch/s, Loss=0.7, Acc=0.724]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6997 Acc: 0.7242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Training: 100%|██████████| 265/265 [02:26<00:00,  1.80batch/s, Loss=0.759, Acc=0.694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20\n",
      "Train Loss: 0.7591 Acc: 0.6942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Validation: 100%|██████████| 67/67 [00:17<00:00,  3.73batch/s, Loss=0.664, Acc=0.739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6637 Acc: 0.7393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Training: 100%|██████████| 265/265 [02:14<00:00,  1.97batch/s, Loss=0.732, Acc=0.706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n",
      "Train Loss: 0.7320 Acc: 0.7065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Validation: 100%|██████████| 67/67 [00:16<00:00,  3.96batch/s, Loss=0.668, Acc=0.737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6682 Acc: 0.7372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Training: 100%|██████████| 265/265 [02:16<00:00,  1.94batch/s, Loss=0.712, Acc=0.722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n",
      "Train Loss: 0.7123 Acc: 0.7220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Validation: 100%|██████████| 67/67 [00:17<00:00,  3.79batch/s, Loss=0.651, Acc=0.74] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6512 Acc: 0.7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Training: 100%|██████████| 265/265 [02:21<00:00,  1.88batch/s, Loss=0.702, Acc=0.72] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20\n",
      "Train Loss: 0.7020 Acc: 0.7196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Validation: 100%|██████████| 67/67 [00:17<00:00,  3.89batch/s, Loss=0.653, Acc=0.737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6527 Acc: 0.7374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Training: 100%|██████████| 265/265 [02:14<00:00,  1.97batch/s, Loss=0.699, Acc=0.722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      "Train Loss: 0.6992 Acc: 0.7224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Validation: 100%|██████████| 67/67 [00:16<00:00,  4.06batch/s, Loss=0.654, Acc=0.743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6538 Acc: 0.7426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Training: 100%|██████████| 265/265 [02:10<00:00,  2.03batch/s, Loss=0.687, Acc=0.731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20\n",
      "Train Loss: 0.6866 Acc: 0.7311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Validation: 100%|██████████| 67/67 [00:16<00:00,  4.03batch/s, Loss=0.622, Acc=0.754]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6215 Acc: 0.7544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Training: 100%|██████████| 265/265 [02:11<00:00,  2.01batch/s, Loss=0.671, Acc=0.731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "Train Loss: 0.6714 Acc: 0.7311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Validation: 100%|██████████| 67/67 [00:16<00:00,  4.02batch/s, Loss=0.608, Acc=0.76] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6084 Acc: 0.7603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Training: 100%|██████████| 265/265 [02:12<00:00,  2.00batch/s, Loss=0.672, Acc=0.735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20\n",
      "Train Loss: 0.6722 Acc: 0.7348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Validation: 100%|██████████| 67/67 [00:17<00:00,  3.87batch/s, Loss=0.616, Acc=0.758]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6156 Acc: 0.7577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Training: 100%|██████████| 265/265 [02:10<00:00,  2.03batch/s, Loss=0.652, Acc=0.745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20\n",
      "Train Loss: 0.6525 Acc: 0.7449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Validation: 100%|██████████| 67/67 [00:16<00:00,  3.99batch/s, Loss=0.594, Acc=0.766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5942 Acc: 0.7662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Training: 100%|██████████| 265/265 [02:12<00:00,  2.00batch/s, Loss=0.655, Acc=0.742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n",
      "Train Loss: 0.6546 Acc: 0.7421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Validation: 100%|██████████| 67/67 [00:17<00:00,  3.87batch/s, Loss=0.617, Acc=0.75] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6168 Acc: 0.7502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Training: 100%|██████████| 265/265 [02:11<00:00,  2.01batch/s, Loss=0.646, Acc=0.745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n",
      "Train Loss: 0.6458 Acc: 0.7455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Validation: 100%|██████████| 67/67 [00:17<00:00,  3.86batch/s, Loss=0.609, Acc=0.762]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6085 Acc: 0.7622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Training: 100%|██████████| 265/265 [02:15<00:00,  1.96batch/s, Loss=0.645, Acc=0.749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20\n",
      "Train Loss: 0.6447 Acc: 0.7490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Validation: 100%|██████████| 67/67 [00:17<00:00,  3.92batch/s, Loss=0.614, Acc=0.756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6142 Acc: 0.7556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Training: 100%|██████████| 265/265 [02:13<00:00,  1.99batch/s, Loss=0.643, Acc=0.748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      "Train Loss: 0.6433 Acc: 0.7478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Validation: 100%|██████████| 67/67 [00:16<00:00,  3.95batch/s, Loss=0.607, Acc=0.766]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6068 Acc: 0.7658\n",
      "Early stopping triggered!\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "patience = 4\n",
    "model_save_path = 'COVID-19_Radiography_Dataset/models/best_model_lbp.pth'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  running_loss = 0.0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  model.train()\n",
    "  \n",
    "  train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\", unit=\"batch\")\n",
    "  for inputs, labels in train_progress:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item() * inputs.size(0)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_progress.set_postfix({\"Loss\": running_loss / total, \"Acc\": correct / total})\n",
    "\n",
    "  epoch_loss = running_loss / len(train_loader.dataset)\n",
    "  epoch_acc = correct / total\n",
    "\n",
    "  print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "  print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "  wandb.log({\"epoch\": epoch + 1, \"train_loss\": epoch_loss, \"train_accuracy\": epoch_acc})\n",
    "\n",
    "  model.eval()\n",
    "  val_loss = 0.0\n",
    "  val_correct = 0\n",
    "  val_total = 0\n",
    "\n",
    "  # Validation phase with progress bar\n",
    "  val_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\", unit=\"batch\")\n",
    "  with torch.no_grad():\n",
    "    for inputs, labels in val_progress:\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      val_loss += loss.item() * inputs.size(0)\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      val_total += labels.size(0)\n",
    "      val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "      val_progress.set_postfix({\"Loss\": val_loss / val_total, \"Acc\": val_correct / val_total})\n",
    "\n",
    "  val_loss = val_loss / len(val_loader.dataset)\n",
    "  val_acc = val_correct / val_total\n",
    "\n",
    "  print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "\n",
    "  wandb.log({\"epoch\": epoch + 1, \"val_loss\": val_loss, \"val_accuracy\": val_acc})\n",
    "\n",
    "  # Check for improvement\n",
    "  if val_loss < best_val_loss:\n",
    "    best_val_loss = val_loss\n",
    "    epochs_no_improve = 0\n",
    "    torch.save(model.state_dict(), model_save_path)  # Save the best model\n",
    "  else:\n",
    "    epochs_no_improve += 1\n",
    "\n",
    "  if epochs_no_improve >= patience:\n",
    "    print(\"Early stopping triggered!\")\n",
    "    break\n",
    "\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
