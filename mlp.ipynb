{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, ops\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'COVID-19_Radiography_Dataset'\n",
    "train_dir = os.path.join(root_dir, 'lbp', 'train')\n",
    "val_dir = os.path.join(root_dir, 'lbp', 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdlizano\u001b[0m (\u001b[33mci-0148-g3\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\ISRAEL\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\ISRAEL\\Documents\\Estudios\\UCR\\Computacion\\MachineLearning\\proyecto2\\lung_classifier_neuralNetworks\\wandb\\run-20240527_013102-t8hlymvr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ci-0148-g3/mlp_local_binary_patterns/runs/t8hlymvr' target=\"_blank\">LBP default</a></strong> to <a href='https://wandb.ai/ci-0148-g3/mlp_local_binary_patterns' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ci-0148-g3/mlp_local_binary_patterns' target=\"_blank\">https://wandb.ai/ci-0148-g3/mlp_local_binary_patterns</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ci-0148-g3/mlp_local_binary_patterns/runs/t8hlymvr' target=\"_blank\">https://wandb.ai/ci-0148-g3/mlp_local_binary_patterns/runs/t8hlymvr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ci-0148-g3/mlp_local_binary_patterns/runs/t8hlymvr?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x21d5cfb9810>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=os.getenv('WANDB_KEY'))\n",
    "wandb.init(project='mlp_local_binary_patterns', name='LBP default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script shown below calculates the mean and standard deviation for the training set. These values are then used to apply the standarization to the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(train_dir, transform=Grayscale())\n",
    "loader = DataLoader(dataset, 64, shuffle=True)\n",
    "\n",
    "# Initialize variables for mean and std calculation\n",
    "mean = 0.0\n",
    "std = 0.0\n",
    "nb_samples = 0\n",
    "\n",
    "for data, _ in loader:\n",
    "    data = data.to(device)\n",
    "    batch_samples = data.size(0)  # number of images in the batch\n",
    "    data = data.view(batch_samples, -1)  # flatten the channel and spatial dimensions\n",
    "    mean += data.mean(1).sum(0)\n",
    "    std += data.std(1).sum(0)\n",
    "    nb_samples += batch_samples\n",
    "\n",
    "mean /= nb_samples\n",
    "std /= nb_samples\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These transforms are applied for standarization and data augmentation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "        transforms.RandomResizedCrop(size=299, scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=0.0197, std=0.0083)  # Normalize the image (uniform LBP)\n",
    "        transforms.Normalize(mean=0.6133, std=0.3372)  # Normalize the image (default LBP)\n",
    "        # transforms.Normalize(mean=0.2283, std=0.3103)  # Normalize the image (ror LBP)\n",
    "        # transforms.Normalize(mean=0.0772, std=0.1784)  # Normalize the image (var LBP)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Grayscale(),\n",
    "        # transforms.Normalize(mean=0.0197, std=0.0083)  # Normalize the (uniform LBP)\n",
    "        transforms.Normalize(mean=0.6133, std=0.3372)  # Normalize the image (default LBP)\n",
    "        # transforms.Normalize(mean=0.2283, std=0.3103)  # Normalize the image (ror LBP)\n",
    "        # transforms.Normalize(mean=0.0772, std=0.1784)  # Normalize the image (var LBP)\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP PyTorch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim, hidden_layers=[128], activation='relu', dropout=0):\n",
    "    super(MLP, self).__init__()\n",
    "\n",
    "    if activation == \"relu\":\n",
    "      activation_func = lambda: nn.ReLU()\n",
    "    elif activation == \"prelu\":\n",
    "      activation_func = lambda: nn.PReLU()\n",
    "    # You can add other activation functions here\n",
    "    else:\n",
    "      raise NotImplementedError(\"Activation function not supported\")\n",
    "\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.input_layer = nn.Linear(input_dim, hidden_layers[0])\n",
    "    self.input_activation = activation_func()\n",
    "\n",
    "    self.hidden_layers = nn.ModuleList([])\n",
    "    for index in range(len(hidden_layers)):\n",
    "        next = index + 1\n",
    "        if next < len(hidden_layers):\n",
    "          self.hidden_layers.append(nn.Linear(hidden_layers[index], hidden_layers[next]))\n",
    "          self.hidden_layers.append(nn.BatchNorm1d(hidden_layers[next]))\n",
    "        else:\n",
    "          self.hidden_layers.append(nn.Linear(hidden_layers[index], hidden_layers[index]))\n",
    "          self.hidden_layers.append(nn.BatchNorm1d(hidden_layers[index]))\n",
    "        self.hidden_layers.append(activation_func())\n",
    "        if dropout > 0:\n",
    "          self.hidden_layers.append(nn.Dropout(dropout))\n",
    "\n",
    "    self.output_layer = nn.Linear(hidden_layers[-1], output_dim)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.flatten(x)\n",
    "    x = self.input_layer(x)\n",
    "    x = self.input_activation(x)\n",
    "\n",
    "    for layer in self.hidden_layers:\n",
    "      x = layer(x)\n",
    "\n",
    "    x = self.output_layer(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions and initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "input_features = 299 * 299\n",
    "hidden_layers = [256, 256]\n",
    "num_classes = 4\n",
    "dropout = 0.5\n",
    "activation = 'prelu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=data_transforms['train'])\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=data_transforms['val'])\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "model = MLP(input_dim=input_features,\n",
    "            hidden_layers=hidden_layers,\n",
    "            output_dim=num_classes,\n",
    "            dropout=dropout,\n",
    "            activation=activation).to(device)\n",
    "\n",
    "# Define loss function and optimizer (replace with your choices)\n",
    "criterion = nn.CrossEntropyLoss()  # Assuming classification task\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Training:   0%|          | 0/265 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Training: 100%|██████████| 265/265 [02:13<00:00,  1.99batch/s, Loss=1.02, Acc=0.56] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train Loss: 1.0205 Acc: 0.5604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Validation: 100%|██████████| 67/67 [00:17<00:00,  3.88batch/s, Loss=0.853, Acc=0.652]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.8535 Acc: 0.6522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Training: 100%|██████████| 265/265 [02:11<00:00,  2.01batch/s, Loss=0.886, Acc=0.636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "Train Loss: 0.8865 Acc: 0.6358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Validation: 100%|██████████| 67/67 [00:16<00:00,  3.98batch/s, Loss=0.789, Acc=0.681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7887 Acc: 0.6815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Training: 100%|██████████| 265/265 [02:11<00:00,  2.02batch/s, Loss=0.824, Acc=0.661]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n",
      "Train Loss: 0.8243 Acc: 0.6610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Validation: 100%|██████████| 67/67 [00:16<00:00,  4.03batch/s, Loss=0.725, Acc=0.706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7246 Acc: 0.7055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Training: 100%|██████████| 265/265 [02:11<00:00,  2.01batch/s, Loss=0.772, Acc=0.687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n",
      "Train Loss: 0.7720 Acc: 0.6865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Validation: 100%|██████████| 67/67 [00:16<00:00,  3.99batch/s, Loss=0.696, Acc=0.725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6961 Acc: 0.7254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Training: 100%|██████████| 265/265 [02:17<00:00,  1.93batch/s, Loss=0.745, Acc=0.705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20\n",
      "Train Loss: 0.7449 Acc: 0.7054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Validation: 100%|██████████| 67/67 [00:17<00:00,  3.94batch/s, Loss=0.661, Acc=0.733]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.6615 Acc: 0.7334\n",
      "Early stopping triggered!\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "patience = 4\n",
    "model_save_path = 'COVID-19_Radiography_Dataset/models/best_model_lbp.pth'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  running_loss = 0.0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  model.train()\n",
    "  \n",
    "  train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\", unit=\"batch\")\n",
    "  for inputs, labels in train_progress:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item() * inputs.size(0)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_progress.set_postfix({\"Loss\": running_loss / total, \"Acc\": correct / total})\n",
    "\n",
    "  epoch_loss = running_loss / len(train_loader.dataset)\n",
    "  epoch_acc = correct / total\n",
    "\n",
    "  print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "  print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "  wandb.log({\"epoch\": epoch + 1, \"train_loss\": epoch_loss, \"train_accuracy\": epoch_acc})\n",
    "\n",
    "  model.eval()\n",
    "  val_loss = 0.0\n",
    "  val_correct = 0\n",
    "  val_total = 0\n",
    "\n",
    "  # Validation phase with progress bar\n",
    "  val_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\", unit=\"batch\")\n",
    "  with torch.no_grad():\n",
    "    for inputs, labels in val_progress:\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      val_loss += loss.item() * inputs.size(0)\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      val_total += labels.size(0)\n",
    "      val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "      val_progress.set_postfix({\"Loss\": val_loss / val_total, \"Acc\": val_correct / val_total})\n",
    "\n",
    "  val_loss = val_loss / len(val_loader.dataset)\n",
    "  val_acc = val_correct / val_total\n",
    "\n",
    "  print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "\n",
    "  wandb.log({\"epoch\": epoch + 1, \"val_loss\": val_loss, \"val_accuracy\": val_acc})\n",
    "\n",
    "  # Check for improvement\n",
    "  if val_loss < best_val_loss:\n",
    "    best_val_loss = val_loss\n",
    "    epochs_no_improve = 0\n",
    "    torch.save(model.state_dict(), model_save_path)  # Save the best model\n",
    "  else:\n",
    "    epochs_no_improve += 1\n",
    "\n",
    "  if epochs_no_improve >= patience:\n",
    "    print(\"Early stopping triggered!\")\n",
    "    break\n",
    "\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
