{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogOSSRqaGiUJ"
      },
      "source": [
        "Daniel Lizano Morales C04285\n",
        "\n",
        "Esteban Castañeda Blanco C01795\n",
        "\n",
        "Israel López Vallecillo C04396\n",
        "\n",
        "Ariel Solís"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j2ml7CZVGfMC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from tqdm import tqdm\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "Yfn264v4lWDu",
        "outputId": "4d788bfe-6454-4a8e-d9a5-b05fcfbfb0ff"
      },
      "outputs": [],
      "source": [
        "#no ejecuten esta celda a menos de que vayan a entrenar\n",
        "wandb.login(key=os.getenv('WANDB_KEY'))\n",
        "wandb.init(project='COVID-19_Radiography_Dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ssl\n",
        "\n",
        "# Deshabilitar verificación SSL\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LurHf5driNnI"
      },
      "source": [
        "## CNN general"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjkVu01ZiTSN"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "num_epochs = 5\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "classes = ['Viral_Pneumonia', 'COVID', 'Normal', 'Lung_Opacity']\n",
        "num_classes = len(classes)\n",
        "root_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/'\n",
        "withoutfilter_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/withoutfilter'\n",
        "withfilter_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/filtradas'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vw0xRivm0_7v"
      },
      "outputs": [],
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((240, 240)),  # Adjust size for EfficientNet-B1\n",
        "        transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
        "        transforms.RandomRotation(10),  # Randomly rotate the image by up to 10 degrees\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Randomly change brightness, contrast, saturation, and hue\n",
        "        transforms.ToTensor(),  # Convert the image to a tensor\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize the image\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((240, 240)),  # Adjust size for EfficientNet-B1\n",
        "        transforms.ToTensor(),  # Convert the image to a tensor\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize the image\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kiauz2NBi_i-"
      },
      "outputs": [],
      "source": [
        "def create_folders(base_dir):\n",
        "  train_dir = os.path.join(base_dir, 'train')\n",
        "  val_dir = os.path.join(base_dir, 'val')\n",
        "\n",
        "  os.makedirs(train_dir, exist_ok=True)\n",
        "  os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "  for folder in [train_dir, val_dir]:\n",
        "    for category in classes:\n",
        "      category_dir = os.path.join(folder, category)\n",
        "      os.makedirs(category_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDuGKLn9kjtv"
      },
      "outputs": [],
      "source": [
        "def set_train_val_images(source_dir, base_dir):\n",
        "\n",
        "  train_dir = os.path.join(base_dir, 'train')\n",
        "  val_dir = os.path.join(base_dir, 'val')\n",
        "\n",
        "  os.makedirs(train_dir, exist_ok=True)\n",
        "  os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "  # Copy data to the correct directories if not already done\n",
        "  for category in classes:\n",
        "    category_path = os.path.join(source_dir, category, 'images')\n",
        "    images = os.listdir(category_path)\n",
        "\n",
        "    # Ensure the category directories exist in train and val directories\n",
        "    train_category_dir = os.path.join(train_dir, category)\n",
        "    val_category_dir = os.path.join(val_dir, category)\n",
        "    os.makedirs(train_category_dir, exist_ok=True)\n",
        "    os.makedirs(val_category_dir, exist_ok=True)\n",
        "\n",
        "    train_images = images[:int(len(images) * 0.8)]\n",
        "    val_images = images[int(len(images) * 0.8):]\n",
        "\n",
        "    # Copy train images\n",
        "    for img in tqdm(train_images, desc=f\"Copying train images for {category}\"):\n",
        "        src_path = os.path.join(category_path, img)\n",
        "        dst_path = os.path.join(train_category_dir, img)\n",
        "        if not os.path.exists(dst_path):\n",
        "            shutil.copy(src_path, dst_path)\n",
        "\n",
        "    # Copy val images\n",
        "    for img in tqdm(val_images, desc=f\"Copying val images for {category}\"):\n",
        "        src_path = os.path.join(category_path, img)\n",
        "        dst_path = os.path.join(val_category_dir, img)\n",
        "        if not os.path.exists(dst_path):\n",
        "            shutil.copy(src_path, dst_path)\n",
        "\n",
        "  # Verify the directory structure\n",
        "  print(\"\\nTrain Directory Structure:\")\n",
        "  for root, dirs, files in os.walk(train_dir):\n",
        "      print(root, \"contains\", len(files), \"files\")\n",
        "\n",
        "  print(\"\\nValidation Directory Structure:\")\n",
        "  for root, dirs, files in os.walk(val_dir):\n",
        "      print(root, \"contains\", len(files), \"files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udjYut_jAqAQ"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, val_loader, device, num_epochs):\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Training phase with progress bar\n",
        "    train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\", unit=\"batch\")\n",
        "    for inputs, labels in train_progress:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item() * inputs.size(0)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "      train_progress.set_postfix({\"Loss\": running_loss / total, \"Acc\": correct / total})\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_acc = correct / total\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "    # Log training metrics to W&B\n",
        "    # wandb.log({\"epoch\": epoch + 1, \"train_loss\": epoch_loss, \"train_accuracy\": epoch_acc})\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    # Validation phase with progress bar\n",
        "    val_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\", unit=\"batch\")\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in val_progress:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        val_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        val_total += labels.size(0)\n",
        "        val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_progress.set_postfix({\"Loss\": val_loss / val_total,\n",
        "                                          \"Acc\": val_correct / val_total})\n",
        "\n",
        "    val_loss = val_loss / len(val_loader.dataset)\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
        "\n",
        "    # Log validation metrics to W&B\n",
        "    #wandb.log({\"epoch\": epoch + 1, \"val_loss\": val_loss, \"val_accuracy\": val_acc})\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUH6EMdD71wH"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, device):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in dataloader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      outputs = model(inputs)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  accuracy = correct / total\n",
        "  print(f'Accuracy: {accuracy:.4f}')\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPjIOdMSiHiW"
      },
      "source": [
        "## CNN sin filtros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NkX-d7dh3G8"
      },
      "outputs": [],
      "source": [
        "withoutfilter_dir = os.path.join(root_dir, 'withoutfilter')\n",
        "os.makedirs(withoutfilter_dir, exist_ok=True)\n",
        "create_folders(withoutfilter_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7WEFbSa2n-o"
      },
      "outputs": [],
      "source": [
        "train_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/withoutfilter/train'\n",
        "val_dir = '/content/drive/MyDrive/COVID-19_Radiography_Dataset/withoutfilter/val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOM9kgPpoEe1",
        "outputId": "c6f45793-799b-47ae-b649-d64d8b793b44"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Copying train images for Viral_Pneumonia: 100%|██████████| 1076/1076 [00:22<00:00, 46.97it/s]\n",
            "Copying val images for Viral_Pneumonia: 100%|██████████| 269/269 [00:03<00:00, 75.15it/s]\n",
            "Copying train images for COVID: 100%|██████████| 2892/2892 [01:30<00:00, 32.07it/s]\n",
            "Copying val images for COVID: 100%|██████████| 724/724 [00:07<00:00, 92.95it/s]\n",
            "Copying train images for Normal: 100%|██████████| 8153/8153 [04:06<00:00, 33.03it/s]\n",
            "Copying val images for Normal: 100%|██████████| 2039/2039 [00:22<00:00, 91.71it/s]\n",
            "Copying train images for Lung_Opacity: 100%|██████████| 4809/4809 [07:11<00:00, 11.15it/s]\n",
            "Copying val images for Lung_Opacity: 100%|██████████| 1203/1203 [00:14<00:00, 84.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Directory Structure:\n",
            "/content/drive/MyDrive/COVID-19_Radiography_Dataset/withoutfilter/train contains 0 files\n",
            "/content/drive/MyDrive/COVID-19_Radiography_Dataset/withoutfilter/train/Viral_Pneumonia contains 1076 files\n",
            "/content/drive/MyDrive/COVID-19_Radiography_Dataset/withoutfilter/train/COVID contains 2892 files\n",
            "/content/drive/MyDrive/COVID-19_Radiography_Dataset/withoutfilter/train/Normal contains 8153 files\n",
            "/content/drive/MyDrive/COVID-19_Radiography_Dataset/withoutfilter/train/Lung_Opacity contains 4809 files\n",
            "\n",
            "Validation Directory Structure:\n",
            "/content/drive/MyDrive/COVID-19_Radiography_Dataset/withoutfilter/val contains 0 files\n",
            "/content/drive/MyDrive/COVID-19_Radiography_Dataset/withoutfilter/val/Viral_Pneumonia contains 269 files\n",
            "/content/drive/MyDrive/COVID-19_Radiography_Dataset/withoutfilter/val/COVID contains 724 files\n",
            "/content/drive/MyDrive/COVID-19_Radiography_Dataset/withoutfilter/val/Normal contains 2039 files\n",
            "/content/drive/MyDrive/COVID-19_Radiography_Dataset/withoutfilter/val/Lung_Opacity contains 1203 files\n"
          ]
        }
      ],
      "source": [
        "set_train_val_images(root_dir, withoutfilter_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmMP4QML09JL"
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=data_transforms['train'])\n",
        "val_dataset = datasets.ImageFolder(val_dir, transform=data_transforms['val'])\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyWc0MYz1El6",
        "outputId": "eead0ed2-ace6-4bc8-dba2-4ad45c5ad394"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b1-f1951068.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b1-f1951068.pth\n",
            "100%|██████████| 30.1M/30.1M [00:00<00:00, 206MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b1\n"
          ]
        }
      ],
      "source": [
        "# Load EfficientNet-B1 model\n",
        "model = EfficientNet.from_pretrained('efficientnet-b1')\n",
        "num_classes = len(train_dataset.classes)\n",
        "model._fc = nn.Linear(model._fc.in_features, num_classes)\n",
        "\n",
        "# Define the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "9Kl-LgSx3RiF",
        "outputId": "31057944-0e8e-4a77-efe8-7f987ccd03fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 - Training: 100%|██████████| 530/530 [05:37<00:00,  1.57batch/s, Loss=0.207, Acc=0.928]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "Train Loss: 0.2074 Acc: 0.9276\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 - Validation: 100%|██████████| 133/133 [18:47<00:00,  8.48s/batch, Loss=0.168, Acc=0.946]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Loss: 0.1682 Acc: 0.9459\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "Error",
          "evalue": "You must call wandb.init() before wandb.log()",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-811b13321151>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-c32d9361d76f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, train_loader, val_loader, device, num_epochs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Log validation metrics to W&B\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val_accuracy\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/preinit.py\u001b[0m in \u001b[0;36mpreinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m ) -> Callable:\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You must call wandb.init() before {name}()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
          ]
        }
      ],
      "source": [
        "trained_model = train_model(model, criterion, optimizer, train_loader, val_loader, device, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmSHFEQa3fb4"
      },
      "outputs": [],
      "source": [
        "val_accuracy = evaluate_model(trained_model, val_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zxpIh5tuHlR"
      },
      "source": [
        "CNN con filtros\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "fuEL3_0jvHWF"
      },
      "outputs": [],
      "source": [
        "base_dir = 'COVID-19_Radiography_Dataset/filtradas' #este path es solo para la cnn con filtro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "htjaRa-Epprp"
      },
      "outputs": [],
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=1),  # Convertir a blanco y negro\n",
        "        transforms.Resize((240, 240)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5])  # Normalizar el canal único\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=1),  # Convertir a blanco y negro\n",
        "        transforms.Resize((240, 240)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5])  # Normalizar el canal único\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "TuYO1ejfEH-_"
      },
      "outputs": [],
      "source": [
        "# Define train and val directories\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')\n",
        "\n",
        "# Create train and val directories if they don't exist\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2ImBjB618Tj",
        "outputId": "be284368-dbca-4a18-8436-cd02ecfa1efe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Directory Structure:\n",
            "COVID-19_Radiography_Dataset/filtradas/train contains 0 files\n",
            "COVID-19_Radiography_Dataset/filtradas/train/Viral_Pneumonia contains 1076 files\n",
            "COVID-19_Radiography_Dataset/filtradas/train/Lung_Opacity contains 4809 files\n",
            "COVID-19_Radiography_Dataset/filtradas/train/Normal contains 8153 files\n",
            "COVID-19_Radiography_Dataset/filtradas/train/Covid contains 2892 files\n",
            "\n",
            "Validation Directory Structure:\n",
            "COVID-19_Radiography_Dataset/filtradas/val contains 0 files\n",
            "COVID-19_Radiography_Dataset/filtradas/val/Viral_Pneumonia contains 269 files\n",
            "COVID-19_Radiography_Dataset/filtradas/val/Lung_Opacity contains 1203 files\n",
            "COVID-19_Radiography_Dataset/filtradas/val/Normal contains 2039 files\n",
            "COVID-19_Radiography_Dataset/filtradas/val/Covid contains 724 files\n"
          ]
        }
      ],
      "source": [
        "# Check if train and val directories are already populated\n",
        "if not os.listdir(train_dir) or not os.listdir(val_dir):\n",
        "    # Copy data to the correct directories if not already done\n",
        "    for category in ['Viral_Pneumonia', 'Covid', 'Normal', 'Lung_Opacity']:\n",
        "        category_path = os.path.join(base_dir, category)\n",
        "        images = os.listdir(category_path)\n",
        "\n",
        "        # Ensure the category directories exist in train and val directories\n",
        "        train_category_dir = os.path.join(train_dir, category)\n",
        "        val_category_dir = os.path.join(val_dir, category)\n",
        "        os.makedirs(train_category_dir, exist_ok=True)\n",
        "        os.makedirs(val_category_dir, exist_ok=True)\n",
        "\n",
        "        train_images = images[:int(len(images) * 0.8)]\n",
        "        val_images = images[int(len(images) * 0.8):]\n",
        "\n",
        "        # Copy train images\n",
        "        for img in tqdm(train_images, desc=f\"Copying train images for {category}\"):\n",
        "            src_path = os.path.join(category_path, img)\n",
        "            dst_path = os.path.join(train_category_dir, img)\n",
        "            if not os.path.exists(dst_path):\n",
        "                shutil.copy(src_path, dst_path)\n",
        "\n",
        "        # Copy val images\n",
        "        for img in tqdm(val_images, desc=f\"Copying val images for {category}\"):\n",
        "            src_path = os.path.join(category_path, img)\n",
        "            dst_path = os.path.join(val_category_dir, img)\n",
        "            if not os.path.exists(dst_path):\n",
        "                shutil.copy(src_path, dst_path)\n",
        "\n",
        "# Verify the directory structure\n",
        "print(\"\\nTrain Directory Structure:\")\n",
        "for root, dirs, files in os.walk(train_dir):\n",
        "    print(root, \"contains\", len(files), \"files\")\n",
        "\n",
        "print(\"\\nValidation Directory Structure:\")\n",
        "for root, dirs, files in os.walk(val_dir):\n",
        "    print(root, \"contains\", len(files), \"files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DfUzy-egLzCy"
      },
      "outputs": [],
      "source": [
        "# Load datasets with data augmentation\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=data_transforms['train'])\n",
        "val_dataset = datasets.ImageFolder(val_dir, transform=data_transforms['val'])\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFo8i65231mG",
        "outputId": "0715a7cd-3dd6-49ef-a625-292898730629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b1\n"
          ]
        }
      ],
      "source": [
        "# Cambiar la primera capa para aceptar un solo canal\n",
        "model = EfficientNet.from_pretrained('efficientnet-b1')\n",
        "num_classes = len(train_dataset.classes)\n",
        "\n",
        "# Cambiar la primera capa para aceptar un solo canal\n",
        "model._conv_stem = nn.Conv2d(1, model._conv_stem.out_channels, \n",
        "                             kernel_size=model._conv_stem.kernel_size, \n",
        "                             stride=model._conv_stem.stride, \n",
        "                             padding=model._conv_stem.padding, \n",
        "                             bias=False)\n",
        "\n",
        "# Cambiar la última capa para el número de clases\n",
        "model._fc = nn.Linear(model._fc.in_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkkALduQvoZG",
        "outputId": "b3190d42-30cf-4642-ef93-5047a1581e65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#wandb.watch(model, log=\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Gpa1uDp1vuur"
      },
      "outputs": [],
      "source": [
        "# Early Stopping parameters\n",
        "patience = 5\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir la ruta para guardar el modelo en Google Drive\n",
        "model_save_path = 'COVID-19_Radiography_Dataset/models/best_model.pth'\n",
        "optimizer_save_path = 'COVID-19_Radiography_Dataset/models/model_and_optimizer.pth'\n",
        "\n",
        "# Crear el directorio si no existe\n",
        "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "etM5gfUfqmhd"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, val_loader, device, num_epochs=50, patience=5):\n",
        "    global best_val_loss, epochs_no_improve\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Training phase with progress bar\n",
        "        train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\", unit=\"batch\")\n",
        "        for inputs, labels in train_progress:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            train_progress.set_postfix({\"Loss\": running_loss / total, \"Acc\": correct / total})\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = correct / total\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        # Log training metrics to W&B\n",
        "        #wandb.log({\"epoch\": epoch + 1, \"train_loss\": epoch_loss, \"train_accuracy\": epoch_acc})\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        # Validation phase with progress bar\n",
        "        val_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\", unit=\"batch\")\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_progress:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                val_progress.set_postfix({\"Loss\": val_loss / val_total, \"Acc\": val_correct / val_total})\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
        "\n",
        "        # Log validation metrics to W&B\n",
        "        #wandb.log({\"epoch\": epoch + 1, \"val_loss\": val_loss, \"val_accuracy\": val_acc})\n",
        "\n",
        "        # Check for improvement\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), model_save_path)  # Save the best model\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "            }, optimizer_save_path)\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        # Save the model after each epoch\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "        }, optimizer_save_path)\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy7hXKWK-Vez",
        "outputId": "19f7d911-6028-4c62-cc0c-42eef0bd1535"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 - Training: 100%|██████████| 530/530 [2:34:43<00:00, 17.52s/batch, Loss=0.449, Acc=0.835]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 0.4488 Acc: 0.8351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 - Validation: 100%|██████████| 133/133 [08:40<00:00,  3.91s/batch, Loss=0.352, Acc=0.868]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Loss: 0.3516 Acc: 0.8680\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20 - Training: 100%|██████████| 530/530 [2:49:34<00:00, 19.20s/batch, Loss=0.289, Acc=0.898]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/20\n",
            "Train Loss: 0.2889 Acc: 0.8979\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20 - Validation: 100%|██████████| 133/133 [09:00<00:00,  4.07s/batch, Loss=0.28, Acc=0.905] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Loss: 0.2801 Acc: 0.9051\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20 - Training:  38%|███▊      | 200/530 [57:28<1:34:50, 17.24s/batch, Loss=0.271, Acc=0.905]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/daniellizano/Desktop/cnn/proyecto2.ipynb Cell 36\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/daniellizano/Desktop/cnn/proyecto2.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trained_model \u001b[39m=\u001b[39m train_model(model, criterion, optimizer, train_loader, val_loader, device, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, patience\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
            "\u001b[1;32m/Users/daniellizano/Desktop/cnn/proyecto2.ipynb Cell 36\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daniellizano/Desktop/cnn/proyecto2.ipynb#X45sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m inputs, labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daniellizano/Desktop/cnn/proyecto2.ipynb#X45sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/daniellizano/Desktop/cnn/proyecto2.ipynb#X45sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daniellizano/Desktop/cnn/proyecto2.ipynb#X45sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daniellizano/Desktop/cnn/proyecto2.ipynb#X45sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
            "File \u001b[0;32m~/Desktop/cnn/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Desktop/cnn/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/cnn/.venv/lib/python3.11/site-packages/efficientnet_pytorch/model.py:314\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"EfficientNet's forward function.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[39m   Calls extract_features to extract features, applies final linear layer, and returns logits.\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39m    Output of this model after processing.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m# Convolution layers\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_features(inputs)\n\u001b[1;32m    315\u001b[0m \u001b[39m# Pooling and final linear layer\u001b[39;00m\n\u001b[1;32m    316\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_avg_pooling(x)\n",
            "File \u001b[0;32m~/Desktop/cnn/.venv/lib/python3.11/site-packages/efficientnet_pytorch/model.py:296\u001b[0m, in \u001b[0;36mEfficientNet.extract_features\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[39mif\u001b[39;00m drop_connect_rate:\n\u001b[1;32m    295\u001b[0m         drop_connect_rate \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(idx) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocks)  \u001b[39m# scale drop connect_rate\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m     x \u001b[39m=\u001b[39m block(x, drop_connect_rate\u001b[39m=\u001b[39;49mdrop_connect_rate)\n\u001b[1;32m    298\u001b[0m \u001b[39m# Head\u001b[39;00m\n\u001b[1;32m    299\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swish(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bn1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv_head(x)))\n",
            "File \u001b[0;32m~/Desktop/cnn/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Desktop/cnn/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/cnn/.venv/lib/python3.11/site-packages/efficientnet_pytorch/model.py:109\u001b[0m, in \u001b[0;36mMBConvBlock.forward\u001b[0;34m(self, inputs, drop_connect_rate)\u001b[0m\n\u001b[1;32m    106\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bn0(x)\n\u001b[1;32m    107\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swish(x)\n\u001b[0;32m--> 109\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_depthwise_conv(x)\n\u001b[1;32m    110\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bn1(x)\n\u001b[1;32m    111\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swish(x)\n",
            "File \u001b[0;32m~/Desktop/cnn/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Desktop/cnn/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/cnn/.venv/lib/python3.11/site-packages/efficientnet_pytorch/utils.py:275\u001b[0m, in \u001b[0;36mConv2dStaticSamePadding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    274\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatic_padding(x)\n\u001b[0;32m--> 275\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mconv2d(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n\u001b[1;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trained_model = train_model(model, criterion, optimizer, train_loader, val_loader, device, num_epochs=20, patience=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0QEav3lyZ8H",
        "outputId": "6b7ad043-f1e4-4900-fbe4-672b2a7978bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the best model for evaluation\n",
        "model_load_path = 'COVID-19_Radiography_Dataset/models/best_model.pth'\n",
        "model.load_state_dict(torch.load(model_load_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WExVHng-_XEV"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  \n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "nQ7F8oNqq2h7",
        "outputId": "44436b1a-5d0d-4afe-d473-e373d6f16b91"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'trained_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-02ccc0d3fc74>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'trained_model' is not defined"
          ]
        }
      ],
      "source": [
        "val_accuracy = evaluate_model(trained_model, val_loader, device)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
